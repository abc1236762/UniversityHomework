{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - IMDB Review (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants, functions and set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 4000\n",
    "MAXLEN = 400\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 8\n",
    "SEED = 0xFFFF\n",
    "SHOULD_RANDOMLY_SPLIT_VAL = False\n",
    "VAL_SPLIT = 0\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "def create_result_dir() -> str:\n",
    "    result_dir = \"result.\" + datetime.today().strftime(\"%y%m%d-%H%M%S\")\n",
    "    if not path.exists(result_dir): os.makedirs(result_dir)\n",
    "    else:\n",
    "        result_dir_suffix = 0\n",
    "        while path.exists(result_dir + \"-\" + str(result_dir_suffix)):\n",
    "            result_dir_suffix += 1\n",
    "        result_dir = result_dir + \"-\" + str(result_dir_suffix)\n",
    "        os.makedirs(result_dir)\n",
    "    return result_dir\n",
    "\n",
    "import pandas, numpy\n",
    "from keras.preprocessing import sequence, text\n",
    "def prepare_data(\n",
    "    train_data: pandas.DataFrame, test_data: pandas.DataFrame,\n",
    ") -> (numpy.ndarray, numpy.ndarray, numpy.ndarray):\n",
    "    train_labels = train_data[\"Sentiment\"].values.astype(\"int32\")\n",
    "    train_texts = train_data[\"SentimentText\"]\n",
    "    test_texts = test_data[\"SentimentText\"]\n",
    "    tokenizer = text.Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    def pad_sequences_from_texts(texts: pandas.Series) \\\n",
    "        -> numpy.ndarray:\n",
    "        return sequence.pad_sequences(\n",
    "            tokenizer.texts_to_sequences(texts),\n",
    "            maxlen=MAXLEN,\n",
    "        )\n",
    "    return (\n",
    "        train_labels,\n",
    "        pad_sequences_from_texts(train_texts),\n",
    "        pad_sequences_from_texts(test_texts),\n",
    "    )\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from keras import callbacks\n",
    "def show_train_history(history: callbacks.History):\n",
    "    fig, ax = pyplot.subplots(nrows=1, ncols=2)\n",
    "    fig.set_size_inches(18, 6)\n",
    "    ax[0].set_title(\"Model accuary\")\n",
    "    ax[0].plot(history.history[\"acc\"])\n",
    "    ax[0].plot(history.history[\"val_acc\"])\n",
    "    ax[0].set_ylabel(\"accuary\")\n",
    "    ax[0].set_xlabel(\"epoch\")\n",
    "    ax[0].legend([\"result\", \"validation\"], loc=\"upper left\")\n",
    "    ax[1].set_title(\"Model loss\")\n",
    "    ax[1].plot(history.history[\"loss\"])\n",
    "    ax[1].plot(history.history[\"val_loss\"])\n",
    "    ax[1].set_ylabel(\"loss\")\n",
    "    ax[1].set_xlabel(\"epoch\")\n",
    "    ax[1].legend([\"result\", \"validation\"], loc=\"upper left\")\n",
    "    pyplot.show()\n",
    "\n",
    "from numpy import random\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from keras.utils import np_utils\n",
    "train = pandas.read_csv(\"data/train_data.csv\")\n",
    "test = pandas.read_csv(\"data/test_data_ans.csv\")\n",
    "train_labels, train_texts, test_texts = prepare_data(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers, initializers, activations\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=NUM_WORDS,\n",
    "        output_dim=16,\n",
    "        input_length=MAXLEN,\n",
    "    ),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.LSTM(16),\n",
    "    layers.Dense(\n",
    "        units=512,\n",
    "        activation=activations.relu,\n",
    "    ),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(\n",
    "        units=1,\n",
    "        activation=activations.sigmoid,\n",
    "    ),\n",
    "])\n",
    "\n",
    "from keras import optimizers, losses\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(amsgrad=True),\n",
    "    loss=losses.binary_crossentropy,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = create_result_dir()\n",
    "model_filename = \"model.{epoch:0%dd}-{val_loss:.4f}.hdf5\" \\\n",
    "    % len(str(abs(EPOCHS)))\n",
    "model_path = path.join(result_dir, model_filename)\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    model_path,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    ")\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=64,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "if SHOULD_RANDOMLY_SPLIT_VAL:\n",
    "    from sklearn import model_selection\n",
    "    train_x, val_x, train_y, val_y = model_selection.train_test_split(\n",
    "        train_texts, train_labels,\n",
    "        test_size=VAL_SPLIT,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    val_data = (val_x, val_y)\n",
    "else:\n",
    "    train_x, train_y = train_texts, train_labels\n",
    "    val_data = None\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        model_checkpoint,\n",
    "        early_stopping,\n",
    "    ],\n",
    "    validation_split=VAL_SPLIT,\n",
    "    validation_data=val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(path.join(result_dir, \"history.pickle\"), \"wb\") as file:\n",
    "    pickle.dump(history.history, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "import glob\n",
    "model_files = sorted(glob.glob(path.join(result_dir, \"model.*-*.hdf5\")))\n",
    "model = models.load_model(model_files[-1])\n",
    "predictions = model.predict_classes(test_texts)\n",
    "\n",
    "pandas.DataFrame(data={\"sentiment\": predictions.flatten()}).to_csv(\n",
    "    path.join(result_dir, \"predictions.csv\"),\n",
    "    index_label=\"id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
